{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "u9lHfIZ-yRVo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOAnXXaKyhZH",
        "outputId": "dd4d6006-2143-44df-c5b0-72ec66a84775"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_labels(file_path):\n",
        "    \"\"\"Load labels from an Excel file.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        print(\"Dataset Loaded Successfully!\")\n",
        "        print(df.head())\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return None\n",
        "\n",
        "# Load dataset\n",
        "labels_df = load_labels(\"/mnt/data/labels..xlsx\")\n",
        "if labels_df is None:\n",
        "    raise FileNotFoundError(\"Labels file is missing! Check file path.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2KI2E-wzD90",
        "outputId": "7b7cc69f-e600-41d9-badf-8d727fe73555"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded Successfully!\n",
            "   Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0      family          genus  \\\n",
            "0             0         68711       70465   Veneridae  Hysteroconcha   \n",
            "1             1          6018        6162   Cardiidae   Acrosterigma   \n",
            "2             2         42500       44103  Pectinidae   Decatopecten   \n",
            "3             3         32647       34008   Mytilidae   Brachidontes   \n",
            "4             4         11386       11538  Carditidae        Cardita   \n",
            "\n",
            "                  species                                           filename  \\\n",
            "0  Hysteroconchalupanaria  Veneridae_Hysteroconcha_lupanaria_allspira_26-...   \n",
            "1  Acrosterigmaattenuatum    Cardiidae_Acrosterigma_attenuatum_bigai_010.jpg   \n",
            "2    Decatopectenamiculum  Pectinidae_Decatopecten_amiculum_Poppe_263539-...   \n",
            "3  Brachidontessemilaevis  Mytilidae_Brachidontes_semilaevis_Poppe_284926...   \n",
            "4     Carditaplanicostata  Carditidae_Cardita_planicostata_PRI_132-view2-...   \n",
            "\n",
            "  ansicht      order          subclass  ... family_idx  genus_idx  \\\n",
            "0  aussen   Venerida      Imparidentia  ...         47        410   \n",
            "1  aussen   Cardiida      Imparidentia  ...         55          8   \n",
            "2  aussen  Pectinida     Pteriomorphia  ...         23        258   \n",
            "3  aussen   Mytilida     Pteriomorphia  ...         12        122   \n",
            "4   innen  Carditida  Archiheterodonta  ...         73        143   \n",
            "\n",
            "   species_idx  ansicht_idx  order_idx  subclass_idx  Index  \\\n",
            "0         1841            0         26             2  70465   \n",
            "1           74            0          2             2   6162   \n",
            "2         1184            0         20             5  44103   \n",
            "3          592            0         15             5  34008   \n",
            "4          702            1          3             1  11538   \n",
            "\n",
            "                                            FilePath ClassNo  FileType  \n",
            "0  IMAGES_final/Veneridae_Hysteroconcha_lupanaria...    1841     image  \n",
            "1  IMAGES_final/Cardiidae_Acrosterigma_attenuatum...      74     image  \n",
            "2  IMAGES_final/Pectinidae_Decatopecten_amiculum_...    1184     image  \n",
            "3  IMAGES_final/Mytilidae_Brachidontes_semilaevis...     592     image  \n",
            "4  IMAGES_final/Carditidae_Cardita_planicostata_P...     702     image  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_rare_classes(df, level, threshold=10):\n",
        "    \"\"\"Find rare classes in a given taxonomic level.\"\"\"\n",
        "    class_counts = df[level].value_counts()\n",
        "    rare_classes = class_counts[class_counts < threshold].index\n",
        "    return list(rare_classes)\n",
        "\n",
        "# Identify rare classes\n",
        "rare_classes = {level: find_rare_classes(labels_df, level) for level in [\"family\", \"order\", \"subclass\"]}\n",
        "print(\"Rare classes:\", rare_classes)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L2bwVd8zRgz",
        "outputId": "57ffed90-8880-40e6-8a85-b23dc8ff5e48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rare classes: {'family': ['Cyamiidae', 'Cleidothaeridae'], 'order': ['Cyamioidea'], 'subclass': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of samples for each family\n",
        "family_counts = labels_df['family'].value_counts()\n",
        "print(\"\\nNumber of samples for each family:\")\n",
        "for family, count in family_counts.items():\n",
        "    print(f\"Family: {family}, Count: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WznUNJgtZpMa",
        "outputId": "991b1a10-b7db-4dd6-bc1b-6f165d83f22d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of samples for each family:\n",
            "Family: Pectinidae, Count: 9293\n",
            "Family: Cardiidae, Count: 5390\n",
            "Family: Tellinidae, Count: 5330\n",
            "Family: Veneridae, Count: 4725\n",
            "Family: Mytilidae, Count: 4141\n",
            "Family: Arcidae, Count: 4038\n",
            "Family: Lucinidae, Count: 3140\n",
            "Family: Mactridae, Count: 2408\n",
            "Family: Spondylidae, Count: 2033\n",
            "Family: Glycymerididae, Count: 1999\n",
            "Family: Carditidae, Count: 1707\n",
            "Family: Limidae, Count: 1552\n",
            "Family: Donacidae, Count: 1518\n",
            "Family: Psammobiidae, Count: 1459\n",
            "Family: Nuculanidae, Count: 1164\n",
            "Family: Semelidae, Count: 1136\n",
            "Family: Ostreidae, Count: 1117\n",
            "Family: Nuculidae, Count: 1055\n",
            "Family: Cuspidariidae, Count: 978\n",
            "Family: Corbulidae, Count: 946\n",
            "Family: Lasaeidae, Count: 944\n",
            "Family: Propeamussiidae, Count: 931\n",
            "Family: Chamidae, Count: 834\n",
            "Family: Pinnidae, Count: 824\n",
            "Family: Pharidae, Count: 799\n",
            "Family: Anomiidae, Count: 761\n",
            "Family: Pholadidae, Count: 757\n",
            "Family: Astartidae, Count: 665\n",
            "Family: Yoldiidae, Count: 663\n",
            "Family: Crassatellidae, Count: 573\n",
            "Family: Glossidae, Count: 508\n",
            "Family: Pteriidae, Count: 472\n",
            "Family: Isognomonidae, Count: 470\n",
            "Family: Thyasiridae, Count: 465\n",
            "Family: Noetiidae, Count: 443\n",
            "Family: Ungulinidae, Count: 431\n",
            "Family: Solecurtidae, Count: 404\n",
            "Family: Solenidae, Count: 403\n",
            "Family: Thraciidae, Count: 387\n",
            "Family: Hiatellidae, Count: 375\n",
            "Family: Myidae, Count: 368\n",
            "Family: Limopsidae, Count: 346\n",
            "Family: Pandoridae, Count: 323\n",
            "Family: Cyrenidae, Count: 306\n",
            "Family: Plicatulidae, Count: 280\n",
            "Family: Lyonsiidae, Count: 203\n",
            "Family: Galeommatidae, Count: 200\n",
            "Family: Vesicomyidae, Count: 199\n",
            "Family: Malleidae, Count: 198\n",
            "Family: Laternulidae, Count: 193\n",
            "Family: Gryphaeidae, Count: 190\n",
            "Family: Malletiidae, Count: 169\n",
            "Family: Poromyidae, Count: 163\n",
            "Family: Solemyidae, Count: 161\n",
            "Family: Trapezidae, Count: 161\n",
            "Family: Periplomatidae, Count: 149\n",
            "Family: Dreissenidae, Count: 132\n",
            "Family: Myochamidae, Count: 120\n",
            "Family: Gaimardiidae, Count: 110\n",
            "Family: Verticordiidae, Count: 87\n",
            "Family: Glauconomidae, Count: 86\n",
            "Family: Teredinidae, Count: 72\n",
            "Family: Gastrochaenidae, Count: 71\n",
            "Family: Arcticidae, Count: 69\n",
            "Family: Trigoniidae, Count: 67\n",
            "Family: Nucinellidae, Count: 61\n",
            "Family: Kelliellidae, Count: 38\n",
            "Family: Hemidonacidae, Count: 37\n",
            "Family: Placunidae, Count: 26\n",
            "Family: Cyrenoididae, Count: 24\n",
            "Family: Dimyidae, Count: 20\n",
            "Family: Pulvinitidae, Count: 11\n",
            "Family: Cyamiidae, Count: 7\n",
            "Family: Cleidothaeridae, Count: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of samples for each family\n",
        "order_counts = labels_df['order'].value_counts()\n",
        "print(\"\\nNumber of samples for each order:\")\n",
        "for order, count in order_counts.items():\n",
        "    print(f\"Order: {order}, Count: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkjNJq0IZp8U",
        "outputId": "175419a4-2c9f-4704-bc8b-1937e8d80d21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of samples for each order:\n",
            "Order: Cardiida, Count: 15237\n",
            "Order: Pectinida, Count: 13344\n",
            "Order: Venerida, Count: 9826\n",
            "Order: Arcida, Count: 6826\n",
            "Order: Mytilida, Count: 4141\n",
            "Order: Lucinida, Count: 3605\n",
            "Order: Ostreida, Count: 3282\n",
            "Order: Carditida, Count: 2945\n",
            "Order: Myida, Count: 2275\n",
            "Order: Nuculanida, Count: 1996\n",
            "Order: Adapedonta, Count: 1577\n",
            "Order: Limida, Count: 1552\n",
            "Order: Galeommatida, Count: 1144\n",
            "Order: Nuculida, Count: 1055\n",
            "Order: Cuspidarioidea, Count: 978\n",
            "Order: Thracioidea, Count: 536\n",
            "Order: Pandoroidea, Count: 526\n",
            "Order: Solemyida, Count: 222\n",
            "Order: Laternuloidea, Count: 193\n",
            "Order: Poromyoidea, Count: 163\n",
            "Order: Myochamoidea, Count: 123\n",
            "Order: Gaimardioidea, Count: 110\n",
            "Order: Verticordioidea, Count: 87\n",
            "Order: Gastrochaenida, Count: 71\n",
            "Order: Trigoniida, Count: 67\n",
            "Order: Cyamioidea, Count: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of samples for each family\n",
        "subclass_counts = labels_df['subclass'].value_counts()\n",
        "print(\"\\nNumber of samples for each subclass:\")\n",
        "for subclass, count in subclass_counts.items():\n",
        "    print(f\"Subclass: {subclass}, Count: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN8GHU6_Zt3f",
        "outputId": "8c49b0bd-c85a-4fb9-fd54-f726f91c1224"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of samples for each subclass:\n",
            "Subclass: Imparidentia, Count: 33852\n",
            "Subclass: Pteriomorphia, Count: 29145\n",
            "Subclass: Protobranchia, Count: 3273\n",
            "Subclass: Archiheterodonta, Count: 2945\n",
            "Subclass: Anomalodesmata, Count: 2606\n",
            "Subclass: Paleoheterodonta, Count: 67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_augmentation_model(shape=(224, 224, 3)):\n",
        "\n",
        "\n",
        "\n",
        "    augmentation_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(shape=shape),\n",
        "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        tf.keras.layers.RandomRotation(0.2),\n",
        "        tf.keras.layers.RandomZoom(0.2),\n",
        "        tf.keras.layers.RandomBrightness(0.1, value_range=[0, 255]),\n",
        "        tf.keras.layers.RandomContrast(0.2),\n",
        "        tf.keras.layers.GaussianNoise(0.02),\n",
        "        tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
        "        tf.keras.layers.RandomCrop(height=shape[0], width=shape[1]),\n",
        "        tf.keras.layers.Lambda(lambda x: x, output_shape=shape)\n",
        "    ])\n",
        "    return augmentation_model\n"
      ],
      "metadata": {
        "id": "GpE0onAJ22Xn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_model = build_augmentation_model(shape=(224, 224, 3))"
      ],
      "metadata": {
        "id": "ackwu57FGdZB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "def augment_image(image_path, augmentation_model):\n",
        "    \"\"\"Apply augmentation to an image.\"\"\"\n",
        "\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, IMG_SIZE)\n",
        "    img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
        "\n",
        "    img = augmentation_model(tf.expand_dims(img, axis=0))\n",
        "\n",
        "    return tf.squeeze(img).numpy()\n",
        "\n",
        "def augment_dataset(df, image_dir, augmentation_model, target_samples=5):\n",
        "    \"\"\"Augment images for rare classes.\"\"\"\n",
        "    augmented_samples = []\n",
        "\n",
        "    for level in [\"order\"]:\n",
        "        for cls in find_rare_classes(df, level):\n",
        "            class_samples = df[df[level] == cls]\n",
        "            num_samples = len(class_samples)\n",
        "\n",
        "            for i in range(target_samples - num_samples):\n",
        "                sample = class_samples.sample(1).iloc[0]\n",
        "                img_path = os.path.join(image_dir, sample[\"filename\"])\n",
        "\n",
        "                aug_img = augment_image(img_path, augmentation_model)\n",
        "                if aug_img is not None:\n",
        "                    aug_filename = f\"aug_{i}_{sample['filename']}\"\n",
        "                    aug_path = os.path.join(image_dir, aug_filename)\n",
        "                    cv2.imwrite(aug_path, cv2.cvtColor(aug_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                    new_sample = sample.copy()\n",
        "                    new_sample[\"filename\"] = aug_filename\n",
        "                    augmented_samples.append(new_sample)\n",
        "\n",
        "    return pd.concat([df, pd.DataFrame(augmented_samples)], ignore_index=True)\n",
        "\n",
        "\n",
        "labels_df = augment_dataset(labels_df, \"/content/drive/MyDrive/IMAGES_final\", augmentation_model)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T_y0ilyTzVg9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(df):\n",
        "    \"\"\"Split dataset into train and test sets.\"\"\"\n",
        "    train_test_sets = {}\n",
        "\n",
        "    for level in [\"order\"]:\n",
        "        train, test = train_test_split(df, test_size=0.2, stratify=df[level], random_state=42)\n",
        "        train_test_sets[level] = (train, test)\n",
        "\n",
        "    return train_test_sets\n",
        "\n",
        "data_splits = split_dataset(labels_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "ksgeV-GR0ERL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_labels(df, level):\n",
        "    \"\"\"Encode categorical labels into numerical values.\"\"\"\n",
        "    df = df.copy()  # Prevent modifying the original DataFrame\n",
        "    encoder = LabelEncoder()\n",
        "    df[level] = encoder.fit_transform(df[level])  # Transform labels\n",
        "    return df, encoder  # Return both the updated DataFrame & encoder\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mO3vSIti3Ey1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def add_gaussian_noise(image, stddev=10.0):  # Adjust stddev for [0,255] scale\n",
        "    \"\"\"Adds Gaussian noise to the image while keeping values in [0,255].\"\"\"\n",
        "    stddev = tf.cast(stddev, tf.float32)  # Ensure dtype compatibility\n",
        "    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=stddev, dtype=image.dtype)\n",
        "    image = tf.clip_by_value(image + noise, 0.0, 255.0)  # ✅ Keep values in [0,255]\n",
        "    return image\n",
        "\n",
        "\n",
        "def augment_rare_samples(df, level, target_samples=1000, stddev=10):\n",
        "    \"\"\"\n",
        "    Augments rare class samples using Gaussian noise until each class reaches 1000 samples.\n",
        "    \"\"\"\n",
        "    df_encoded, encoder = encode_labels(df, level)\n",
        "\n",
        "    # Count occurrences of each class\n",
        "    class_counts = df_encoded[level].value_counts()\n",
        "    rare_classes = class_counts[class_counts < target_samples].index.tolist()\n",
        "\n",
        "    augmented_samples = []  # ✅ Store new augmented samples\n",
        "\n",
        "    for rare_class in rare_classes:\n",
        "        rare_class_samples = df_encoded[df_encoded[level] == rare_class]\n",
        "        num_existing = len(rare_class_samples)\n",
        "        num_needed = target_samples - num_existing  # ✅ How many more samples are needed?\n",
        "\n",
        "        # ✅ Sample images from the rare class (with replacement)\n",
        "        sampled_images = rare_class_samples.sample(num_needed, replace=True)\n",
        "\n",
        "        for _, row in sampled_images.iterrows():\n",
        "            augmented_samples.append({\n",
        "                \"filename\": row[\"filename\"],  # ✅ Uses original filename\n",
        "                level: rare_class,  # ✅ Maintains class label\n",
        "            })\n",
        "\n",
        "    # ✅ Convert augmented samples to DataFrame\n",
        "    df_augmented = pd.DataFrame(augmented_samples)\n",
        "\n",
        "    # ✅ Merge with original dataset\n",
        "    df_final = pd.concat([df_encoded, df_augmented], ignore_index=True)\n",
        "\n",
        "    return df_final\n",
        "\n",
        "def create_stratified_dataset(df, image_dir, level, batch_size=128, stddev=10.0):\n",
        "    \"\"\"\n",
        "    Creates a tf.data.Dataset where each batch contains samples from all 74 classes.\n",
        "    \"\"\"\n",
        "    df_balanced = augment_rare_samples(df, level, stddev=stddev)  # ✅ Use the fixed stddev\n",
        "\n",
        "    filepaths = df_balanced[\"filename\"].apply(lambda x: os.path.join(image_dir, x))\n",
        "    labels = df_balanced[level].values.astype(np.int64)  # ✅ Ensure correct dtype\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
        "\n",
        "    # ✅ Define preprocessing function inside create_stratified_dataset()\n",
        "    def load_and_preprocess(filepath, label):\n",
        "        \"\"\"Loads, preprocesses, and applies noise conditionally.\"\"\"\n",
        "        image = tf.io.read_file(filepath)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "        image = tf.image.resize(image, (224, 224))  # ✅ Resizing added\n",
        "        image = tf.cast(image, tf.float32)\n",
        "\n",
        "        # ✅ Apply noise augmentation only to duplicated samples\n",
        "        image = tf.cond(\n",
        "            tf.equal(tf.reduce_sum(tf.where(labels == label)), 0),  # Check if it's an augmented sample\n",
        "            lambda: add_gaussian_noise(image, stddev),  # ✅ Apply Gaussian noise\n",
        "            lambda: image  # ✅ Keep original images unchanged\n",
        "        )\n",
        "        return image, label\n",
        "\n",
        "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # ** Create a separate dataset for each class **\n",
        "    class_datasets = []\n",
        "    unique_classes = np.unique(labels)\n",
        "\n",
        "    for class_label in unique_classes:\n",
        "        class_subset = dataset.filter(lambda x, y: tf.equal(y, class_label))\n",
        "        class_datasets.append(class_subset)\n",
        "\n",
        "    # ** Ensure each batch contains at least one sample from each class **\n",
        "    stratified_dataset = tf.data.Dataset.sample_from_datasets(\n",
        "        class_datasets, weights=[1/len(unique_classes)]*len(unique_classes)\n",
        "    )\n",
        "\n",
        "    # ✅ **Batch AFTER applying augmentation**\n",
        "    stratified_dataset = stratified_dataset.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return stratified_dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gceZENrAwSxw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    \"\"\"Load and preprocess an image.\"\"\"\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    return image\n",
        "\n",
        "def create_dataset(df, image_dir, level):\n",
        "    \"\"\"Create TensorFlow dataset from file paths.\"\"\"\n",
        "    filepaths = [os.path.join(image_dir, fname) for fname in df[\"filename\"]]\n",
        "    df, _ = encode_labels(df, level)\n",
        "    labels = df[level].values  # Ensure labels are properly encoded\n",
        "\n",
        "\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
        "    dataset = dataset.map(lambda x, y: (preprocess_image(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z5Vg4kep084P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_stratified_dataset(data_splits[\"order\"][0], \"/content/drive/MyDrive/IMAGES_final\", \"order\")\n",
        "test_dataset = create_dataset(data_splits[\"order\"][1], \"/content/drive/MyDrive/IMAGES_final\", \"order\")"
      ],
      "metadata": {
        "id": "Mzk6ZPc7wgbO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(\n",
        "    lambda x, y: (tf.cast(x, tf.uint8), tf.cast(y, tf.int64))\n",
        ")\n",
        "\n",
        "test_dataset = test_dataset.map(\n",
        "    lambda x, y: (tf.cast(x, tf.uint8), tf.cast(y, tf.int64))\n",
        ")\n"
      ],
      "metadata": {
        "id": "Qw14Q9t2yh2C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def get_label_counts(dataset, num_classes):\n",
        "    \"\"\"\n",
        "    Accumulate label counts in a single pass over the dataset\n",
        "    using tf.math.bincount.\n",
        "    \"\"\"\n",
        "    # First map out just the labels.\n",
        "    only_labels_ds = dataset.map(lambda x, y: tf.cast(y, tf.int64))\n",
        "\n",
        "    # Initialize the accumulator as int64\n",
        "    initial_state = tf.zeros([num_classes], dtype=tf.int64)\n",
        "\n",
        "    # Reduce over the dataset, batching the counts with bincount.\n",
        "    counts = only_labels_ds.reduce(\n",
        "        initial_state,\n",
        "        lambda accumulated_counts, labels:\n",
        "            accumulated_counts + tf.math.bincount(\n",
        "                tf.reshape(labels, [-1]),\n",
        "                minlength=num_classes,\n",
        "                dtype=tf.int64  # <--- make sure output is int64\n",
        "            )\n",
        "    )\n",
        "    return counts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7z0suDdYyaM0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# If you know there are, say, 10 classes:\n",
        "num_classes = 26\n",
        "label_counts = get_label_counts(train_dataset, num_classes)\n",
        "\n",
        "# Convert to NumPy if you need Python ints for a summary:\n",
        "label_counts_np = label_counts.numpy()\n",
        "\n",
        "# Print the distribution\n",
        "for label, count in enumerate(label_counts_np):\n",
        "    print(f\"Label {label}: {count} samples\")"
      ],
      "metadata": {
        "id": "VLY66beaV7g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shape of the first batch\n",
        "for batch_images, batch_labels in train_dataset.take(1):\n",
        "    print(f\"Train Dataset - Batch Images Shape: {batch_images.shape}\")\n",
        "    print(f\"Train Dataset - Batch Labels Shape: {batch_labels.shape}\")\n",
        "    print(f\"Train Dataset - Batch Images dtype: {batch_images.dtype}\")\n",
        "    print(f\"Train Dataset - Batch Labels dtype: {batch_labels.dtype}\")\n",
        "\n",
        "for batch_images, batch_labels in test_dataset.take(1):\n",
        "    print(f\"Test Dataset - Batch Images Shape: {batch_images.shape}\")\n",
        "    print(f\"Test Dataset - Batch Labels Shape: {batch_labels.shape}\")\n",
        "    print(f\"Test Dataset - Batch Images dtype: {batch_images.dtype}\")\n",
        "    print(f\"Test Dataset - Batch Labels dtype: {batch_labels.dtype}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eluaLhrCpqd",
        "outputId": "dec8b24f-36ab-4fe2-a2be-89b7bdc56476"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset - Batch Images Shape: (128, 224, 224, 3)\n",
            "Train Dataset - Batch Labels Shape: (128,)\n",
            "Test Dataset - Batch Images Shape: (128, 224, 224, 3)\n",
            "Test Dataset - Batch Labels Shape: (128,)\n",
            "Train Dataset - Batch Images dtype: <dtype: 'uint8'>\n",
            "Train Dataset - Batch Labels dtype: <dtype: 'int64'>\n",
            "Test Dataset - Batch Images dtype: <dtype: 'uint8'>\n",
            "Test Dataset - Batch Labels dtype: <dtype: 'int64'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DvMhx1rtr28z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_feature_extraction_model(shape=(224, 224, 3), summary=True):\n",
        "    \"\"\"Builds a ConvNeXtTiny feature extractor with a fixed input shape.\"\"\"\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=shape)\n",
        "    x = augmentation_model(inputs)\n",
        "\n",
        "    pre_trained_model = tf.keras.applications.ConvNeXtTiny(\n",
        "\n",
        "      include_top=False,\n",
        "      weights='imagenet',\n",
        "      input_shape=shape,\n",
        "      pooling=None\n",
        "    )\n",
        "    x = pre_trained_model(x)\n",
        "    x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "    feature_extraction_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "    # Freeze pre-trained model\n",
        "    pre_trained_model.trainable = False\n",
        "    if summary:\n",
        "        print(feature_extraction_model.summary())\n",
        "\n",
        "\n",
        "    return feature_extraction_model\n",
        "\n"
      ],
      "metadata": {
        "id": "7K7vE4WUr7Uu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = IMG_SIZE + (3,)\n",
        "feature_extraction_model = build_feature_extraction_model(INPUT_SHAPE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "mdgr1xZJr8PF",
        "outputId": "4c653e7e-bd2e-45fd-d89a-053ea125fb91"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/convnext/convnext_tiny_notop.h5\n",
            "\u001b[1m111650432/111650432\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ sequential (\u001b[38;5;33mSequential\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ convnext_tiny (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m768\u001b[0m)           │      \u001b[38;5;34m27,820,128\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_max_pooling2d                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)                 │                             │                 │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ convnext_tiny (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">27,820,128</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_max_pooling2d                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)                 │                             │                 │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,820,128\u001b[0m (106.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,820,128</span> (106.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m27,820,128\u001b[0m (106.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,820,128</span> (106.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def batched_feature_extraction(model, image_batch):\n",
        "    # Single forward pass with training=True\n",
        "    return model(image_batch, training=True)\n",
        "\n",
        "def extract_features_in_batches(model, dataset, repetitions):\n",
        "    X, Y = [], []\n",
        "    for i in range(repetitions):\n",
        "        if i:\n",
        "            print(\"\\nRepetition\", i)\n",
        "        for batch_idx, (image_batch, label_batch) in enumerate(dataset):\n",
        "            Y.extend(label_batch)\n",
        "            # Single call that can be retraced once if shapes/dtypes are stable\n",
        "            features = batched_feature_extraction(model, image_batch)\n",
        "            X.extend(features)\n",
        "    return np.array(X), np.array(Y)\n"
      ],
      "metadata": {
        "id": "pGpASKCKvpi4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_with_predict(model, dataset):\n",
        "    # Keras handles batching automatically in a single pass\n",
        "    X = model.predict(dataset, verbose=1)\n",
        "    # Gather labels\n",
        "    Y_list = []\n",
        "    for _, labels in dataset:\n",
        "        Y_list.append(labels)\n",
        "    Y = tf.concat(Y_list, axis=0)\n",
        "    return X, Y.numpy()\n"
      ],
      "metadata": {
        "id": "lsaohNyzsGMX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wp3uYNWFF2Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### START YOUR CODE HERE ###  (≈2 LOC)\n",
        "\n",
        "# Training features and labels\n",
        "train_features, train_labels = extract_features_in_batches(feature_extraction_model, train_dataset, repetitions=1 )\n",
        "\n",
        "# Test features and labels\n",
        "test_features, test_labels = extract_features_with_predict(feature_extraction_model, test_dataset)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdKnH3gS4ffF",
        "outputId": "b3af3d14-031a-4c92-a1b6-3cf7d3f95298"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 2s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of train_features:\", train_features.shape)\n",
        "print(\"Number of samples:\", len(train_features))\n",
        "\n",
        "print(\"Shape of train_features:\", test_features.shape)\n",
        "print(\"Number of samples:\", len(test_features))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbVqe57dk34X",
        "outputId": "b2c62c1b-301e-4185-d5dc-c1fba812bd81"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_features: (67200, 768)\n",
            "Number of samples: 67200\n",
            "Shape of train_features: (14378, 768)\n",
            "Number of samples: 14378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --Insert SMOTE Code Here--\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(sampling_strategy=\"auto\", random_state=42)  # auto = balances all minority classes\n",
        "\n",
        "# Apply SMOTE to training data (do not apply to test data!)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(train_features, train_labels)\n",
        "\n",
        "# Check new class distribution\n",
        "print(\"Class distribution before SMOTE:\", np.bincount(train_labels))\n",
        "print(\"Class distribution after SMOTE:\", np.bincount(y_train_smote))\n",
        "# --End of SMOTE Code--\n",
        "\n",
        "print(\"Shape of train_features:\", X_train_smote.shape)\n",
        "print(\"Number of samples:\", len(X_train_smote))\n",
        "\n",
        "print(\"Shape of train_labels:\", y_train_smote.shape)\n",
        "print(\"Number of samples:\", len(y_train_smote))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MqcGuAI_0TU",
        "outputId": "dc9ab97d-ae67-43cd-b0c9-25f92318d1de"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before SMOTE: [ 1261  5461 12105  2356  1000  1000  1000  1000  1000  1000  1242  2884\n",
            "  1820  1000  3313  1597  1000  2625  1000 10675  1000  1000  1000  1000\n",
            "  7861  1000]\n",
            "Class distribution after SMOTE: [12105 12105 12105 12105 12105 12105 12105 12105 12105 12105 12105 12105\n",
            " 12105 12105 12105 12105 12105 12105 12105 12105 12105 12105 12105 12105\n",
            " 12105 12105]\n",
            "Shape of train_features: (67200, 768)\n",
            "Number of samples: 67200\n",
            "Shape of train_features: (14378, 768)\n",
            "Number of samples: 14378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "predictions = rf.predict(test_features)\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "print(classification_report(test_labels, predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjyOqG7d4EOO",
        "outputId": "a1b5e8f4-2529-4eb2-87b4-d791c33983ec"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.3547085825566838\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.17      0.27       316\n",
            "           1       0.29      0.28      0.29      1365\n",
            "           2       0.40      0.44      0.42      3048\n",
            "           3       0.17      0.35      0.23       589\n",
            "           4       0.26      0.19      0.22       196\n",
            "           5       0.00      0.00      0.00         1\n",
            "           6       0.09      0.55      0.15        22\n",
            "           7       0.18      0.09      0.12       229\n",
            "           8       0.04      0.36      0.07        14\n",
            "           9       0.10      0.46      0.17        39\n",
            "          10       0.26      0.15      0.19       310\n",
            "          11       0.20      0.21      0.20       721\n",
            "          12       0.14      0.04      0.06       455\n",
            "          13       0.02      0.08      0.03        25\n",
            "          14       0.49      0.25      0.33       828\n",
            "          15       0.12      0.06      0.08       399\n",
            "          16       0.10      0.07      0.08       211\n",
            "          17       0.44      0.34      0.38       657\n",
            "          18       0.10      0.08      0.09       105\n",
            "          19       0.65      0.65      0.65      2669\n",
            "          20       0.05      0.36      0.09        33\n",
            "          21       0.28      0.18      0.22        44\n",
            "          22       0.03      0.04      0.03       107\n",
            "          23       0.04      0.54      0.07        13\n",
            "          24       0.31      0.29      0.30      1965\n",
            "          25       0.05      0.18      0.08        17\n",
            "\n",
            "    accuracy                           0.35     14378\n",
            "   macro avg       0.21      0.25      0.19     14378\n",
            "weighted avg       0.38      0.35      0.36     14378\n",
            "\n"
          ]
        }
      ]
    }
  ]
}